#!/usr/bin/env python3
"""
Test script to compare different neural network architectures for stock forecasting
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from main import create_hybrid_cnn_lstm_transformer_model, ARCHITECTURE_CONFIG
import matplotlib.pyplot as plt

def generate_synthetic_data(n_samples=1000, n_features=1):
    """Generate synthetic time series data for testing"""
    # Create a synthetic stock-like time series
    np.random.seed(42)
    trend = np.linspace(100, 200, n_samples)
    noise = np.random.normal(0, 5, n_samples)
    seasonal = 10 * np.sin(np.linspace(0, 4*np.pi, n_samples))
    
    data = trend + noise + seasonal
    return data.reshape(-1, n_features)

def test_architecture(architecture_type, input_shape=(60, 1)):
    """Test a specific architecture"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Testing {architecture_type.upper()} Architecture")
    print(f"{'='*60}")
    
    # Create model
    model = create_hybrid_cnn_lstm_transformer_model(input_shape, architecture_type)
    
    # Compile model
    if architecture_type in ['hybrid', 'transformer_only']:
        from keras.optimizers import AdamW
        optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)
    else:
        optimizer = 'adam'
    
    model.compile(
        optimizer=optimizer,
        loss='mean_squared_error',
        metrics=['mean_absolute_error']
    )
    
    # Print model summary
    print(f"ğŸ“Š Model Summary:")
    model.summary()
    
    # Generate test data
    X_test = np.random.randn(10, *input_shape)
    y_test = np.random.randn(10, 1)
    
    # Test prediction
    try:
        predictions = model.predict(X_test, verbose=0)
        print(f"âœ… Prediction successful! Output shape: {predictions.shape}")
        
        # Calculate some basic metrics
        mae = np.mean(np.abs(predictions - y_test))
        print(f"ğŸ“ˆ Test MAE: {mae:.4f}")
        
        return True, mae
        
    except Exception as e:
        print(f"âŒ Prediction failed: {e}")
        return False, None

def compare_architectures():
    """Compare all available architectures"""
    architectures = ['hybrid', 'cnn_lstm', 'transformer_only', 'original_lstm']
    results = {}
    
    print("ğŸš€ Starting Architecture Comparison Test")
    print("ğŸ—ï¸  HYBRID CNN-LSTM-Transformer is the DEFAULT architecture")
    print("="*80)
    
    for arch in architectures:
        success, mae = test_architecture(arch)
        results[arch] = {'success': success, 'mae': mae}
    
    # Print comparison results
    print(f"\n{'='*80}")
    print("ğŸ“Š ARCHITECTURE COMPARISON RESULTS")
    print(f"{'='*80}")
    
    for arch, result in results.items():
        status = "âœ… SUCCESS" if result['success'] else "âŒ FAILED"
        mae_str = f"MAE: {result['mae']:.4f}" if result['mae'] is not None else "N/A"
        print(f"{arch.upper():<20} | {status:<12} | {mae_str}")
    
    return results

def visualize_architecture_comparison():
    """Create a visual comparison of architectures"""
    architectures = ['Hybrid CNN-LSTM-Transformer', 'CNN-LSTM', 'Transformer Only', 'Original LSTM']
    complexity_scores = [95, 70, 60, 85]  # Relative complexity scores
    expected_performance = [90, 75, 80, 70]  # Expected performance scores
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Complexity comparison
    bars1 = ax1.bar(architectures, complexity_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
    ax1.set_title('Architecture Complexity Comparison', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Complexity Score', fontsize=12)
    ax1.set_ylim(0, 100)
    
    # Add value labels on bars
    for bar, score in zip(bars1, complexity_scores):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                str(score), ha='center', va='bottom', fontweight='bold')
    
    # Expected performance comparison
    bars2 = ax2.bar(architectures, expected_performance, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
    ax2.set_title('Expected Performance Comparison', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Performance Score', fontsize=12)
    ax2.set_ylim(0, 100)
    
    # Add value labels on bars
    for bar, score in zip(bars2, expected_performance):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                str(score), ha='center', va='bottom', fontweight='bold')
    
    # Rotate x-axis labels for better readability
    for ax in [ax1, ax2]:
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('architecture_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š Architecture comparison chart saved as 'architecture_comparison.png'")

if __name__ == "__main__":
    print("ğŸ§ª Neural Network Architecture Testing Suite")
    print("="*80)
    
    # Test all architectures
    results = compare_architectures()
    
    # Create visualization
    visualize_architecture_comparison()
    
    print(f"\nğŸ¯ Current Configuration (DEFAULT: HYBRID):")
    print(f"Architecture Type: {ARCHITECTURE_CONFIG['type']} â­ DEFAULT")
    print(f"CNN Filters: {ARCHITECTURE_CONFIG['cnn_filters']}")
    print(f"LSTM Units: {ARCHITECTURE_CONFIG['lstm_units']}")
    print(f"Transformer Heads: {ARCHITECTURE_CONFIG['transformer_heads']}")
    print(f"Batch Size: {ARCHITECTURE_CONFIG['batch_size']}")
    print(f"Epochs: {ARCHITECTURE_CONFIG['epochs']}")
    
    print(f"\nğŸ’¡ HYBRID CNN-LSTM-Transformer is now the DEFAULT architecture!")
    print(f"ğŸ”§ To change architecture, modify ARCHITECTURE_CONFIG['type'] in main.py")
    print(f"ğŸ“‹ Available options: 'hybrid' (DEFAULT), 'cnn_lstm', 'transformer_only', 'original_lstm'")